\documentclass[a4paper, 12pt]{article}
\input{commandspreamble.tex}
\usepackage{tabularx}

\title{Code documentation}
\author{C\'esar Garro-Mar\'in\thanks{Boston University, email: \href{mailto:cesarlgm@bu.edu}{cesarlgm@bu.edu}}} 

\begin{document}
\maketitle
\section{Notation}
Our GMM model is based on the moment equations:
\beqns
\expect{\psi(w_i,\mu)}{}=0	
\eeqns
where:
\bitem
\item $w_i$ is the vector of data for observation $i$.
\item $\psi$ is a $P\times 1$ vector of functions.
\item $\mu$ is an $R\times 1$ vector of parameters. 
\eitem 


We estimate the model's parameters $\mu$ by solving he problem:
\beqns
\hat{\mu}=\arg\min_c\left(\frac{1}{N}\sum_{i=1}^{N}\psi(w_i,c)\right)'A\left(\frac{1}{N}\sum_{i=1}^{N}\psi(w_i,c)\right)
\eeqns
which we can write as:
\beqns
\hat{\mu}=\arg\min_c\left(\frac{1}{N}\varepsilon(c)'Z\right)A\left(\frac{1}{N}Z'\varepsilon(c)\right)
\eeqns
where $\varepsilon(c)$ is the $N\times 1$ vector of errors of the model, and $Z$ is the $N\times P$ matrix of instruments. Here $N=N_1+N_2+N_3$ is the total number of observations and $N_i$ denotes the number of observations that belong to equation $i$. 

$\varepsilon(c)$ is defined as follows. Define the functions $J(l)$, $E(l)$, $T(l)$, and $I(l)$ which return the job, education level, year, and skill that correspond to observation $l$. In addition, for observations belonging to the employment equation, define as $EET(l)$ as the function that returns the education pair-year cell of the observation. Then the error for observation $l$ is:
\beqns
\varepsilon_l(\mu)=\left\{\begin{array}{cc}
	\Delta \overline{ \ln S_{ijet+1}}-\sum_k \theta_{ke}\overline{S_{kejt}} \pi_{kjt}+\pi_{ijt} & l\leq N_1 \\
	1-\sum_k\theta_{ke}\overline{S_{kejt}}    & N_1<l\leq N_1+N_2	 \\
	\Delta  \left[\ln \frac{q_{ejt+1}}{q_{e'jt+1}}\right]-\beta_j\left[\sum_k\left(\theta_{ke} \overline{S_{kjet}}-\theta_{ke'} \overline{S_{kje' t}}\right)\pi_{kjt}\right]-\gamma_{e,e't} & N_1+N_2<l\leq N 
\end{array}\right.
\eeqns

\section{GMM standard errors}
The GMM estimates are distributed as:
\beqns
\sqrt{N}(\hat\mu-\mu)\rightarrow N(0,\tilde{V})
\eeqns
where $\bar{V}=(D'AD)^{-1}D'AVAD(D'AD)^{-1}$. Here:
\bitem
	\item $D$ is the model gradient.
	\item $V$ is defined as:
	\beqns
		V=\lim_{N\rightarrow\infty}\frac{1}{{N}}\sum_{i=1}^N\expect{\psi(w_i,\mu)\psi(w_i,\mu)'}{}
	\eeqns
	\item $A$ is the weighting matrix $(Z'Z)^{-1}$
\eitem
We estimate $\bar{V}$ as:
\beqns
	\hat{\bar{V}}=(\hat{D}'A\hat{D})^{-1}\hat{D}'A\hat{V}A\hat{D}(\hat{D}'A\hat{D})^{-1}
\eeqns
where:
\bitem 
	\item $\hat{V}=\frac{1}{{N}}\sum_{i=1}^N\psi(w_i,\hat{\mu})\psi(w_i,\hat{\mu})'$.
	\item $\hat{D}=\frac{1}{N}\sum_{i=1}^{N}\frac{\partial \psi(w_i,\hat{\mu})}{\partial c'}$
\eitem
\subsection{Computing the gradient:}
Let $\Xi$ be the $N\times R$ matrix with general term $\Xi_{lr}=\frac{\partial \varepsilon(w_l,c)}{\partial c_{r}}$. Then our estimate of the gradient is $P\times R$ matrix:
\beqns
\hat{D}=\frac{1}{N}Z'\Xi
\eeqns
Thus, we jut have to compute $\Xi_{lr}$. We need to compute derivatives with respect to four types of parameters: $\theta_{ke}$, $\pi_{kjt}$, and $\beta_j$:
\beqns
\frac{\partial \varepsilon_l(\mu)}{\partial \theta_{ie}}&=&\left\{\begin{array}{lc}
	- \overline{S_{kejt}} \pi_{ijt} & l\leq N_1, e=E(l), i\neq 1 \\
	-\overline{S_{iejt}}    & N_1<l\leq N_1+N_2,	e=E(l), i\neq 1 \\
	-\beta_j \overline{S_{ije t}}\pi_{ijt} & N_1+N_2<l\leq N, (e,\cdot,\cdot)=EET(l), i\neq 1\\
	\beta_j  \overline{S_{ije t}}\pi_{ijt} & N_1+N_2<l\leq N, (\cdot,e,\cdot)=EET(l), i\neq 1\\
	0 & \text{otherwise}
\end{array}\right.\\
\frac{\partial \varepsilon_l(\mu)}{\partial \theta_{1}}&=&\left\{\begin{array}{lc}
	- \overline{S_{kejt}} \pi_{ijt} & l\leq N_1\\
	-\overline{S_{iejt}}    & N_1<l\leq N_1+N_2\\
	\beta_j  \overline{S_{ije t}}\pi_{ijt} 	-\beta_j \overline{S_{ije' t}}\pi_{ijt} & N_1+N_2<l\leq N\\
	0 & \text{otherwise}
\end{array}\right.\\
\frac{\partial\varepsilon_l(\mu)}{\partial \pi_{ijt}}&=&\left\{\begin{array}{lc}
	- \theta_{ie}\overline{S_{iejt}} & l\leq N_1, i\neq I(l), j=J(l), t=T(l) \\
	- \theta_{ie}\overline{S_{iejt}}+1 & l\leq N_1, i= I(l),j=J(l), t=T(l) \\
	-\beta_j\left(\theta_{ie} \overline{S_{ijet}}-\theta_{ie'} \overline{S_{ije' t}}\right) & N_1+N_2<l\leq N \\
	0 & \text{otherwise}
\end{array}\right.\\
\frac{\partial\varepsilon_l(\mu)}{\partial \beta_j}&=&\left\{\begin{array}{lc}
	-\left[\sum_k\left(\theta_{ke} \overline{S_{kjet}}-\theta_{ke'} \overline{S_{kje' t}}\right)\pi_{kjt}\right] & N_1+N_2<l\leq N, j=J(l)\\
	0& \text{otherwise}
\end{array}\right.\\
\frac{\partial\varepsilon_l(\mu)}{\partial \gamma_{ee't}}&=&\left\{\begin{array}{lc}
	-1 & N_1+N_2<l\leq N, (e,e',t)=EET(l)\\
	0&  \text{otherwise}
\end{array}\right.\\
\eeqns
We compute $\Xi$ with the function {\tt get\_xi\_matrix}, and compute $\bar{V}$ with {\tt estimate\_v}. Finally, the function {\tt get\_standard\_errors} computes the standard errors.


\subsection{Standard errors for $\sigma_j$ and $d\ln A_{ijt}$}
The above derivation gives standard errors for $\beta_j=\frac{\sigma_j-1}{\sigma_j}$. Using the delta method, it is straightforward to compute standard errors for $\sigma_j=\frac{1}{1-\beta_j}$.
\beqns
	\sqrt{N}(\hat{\sigma_j}-\sigma_j)\rightarrow N\left(0,\text{var}(\sigma_j)\left[\frac{\partial \sigma_j}{\partial \beta_j }\right]^2\right)
\eeqns
where:
\beqns
	\left[\frac{\partial \sigma_j}{\partial \beta_j }\right]^2=\frac{1}{(1-\beta_j)^2}
\eeqns
By an analogous argument:
\beqns
	\sqrt{N}(\hat{d\ln A_{ijt}}-d\ln A_{ijt})\rightarrow N\left(0,G(\theta)'\tilde{V}G(\theta)\right)
\eeqns
where $\tilde{V}$ is the variance matrix of $\theta=(\pi_{ijt},\beta_j)'$ and $	G(\theta)=(\beta_j, \pi_{ijt})'$

\end{document}

